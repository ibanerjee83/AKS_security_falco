az account set --subscription subscription_number
az aks get-credentials --resource-group Rsg --name Akscluster


helm repo add falcosecurity https://falcosecurity.github.io/charts


testserver ~/.kube
$ helm repo add falcosecurity https://falcosecurity.github.io/charts
"falcosecurity" has been added to your repositories

testserver ~/.kube
$ helm repo list
NAME            URL
falcosecurity   https://falcosecurity.github.io/charts



helm repo update

$ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "falcosecurity" chart repository
Update Complete. ⎈ Happy Helming!⎈

testserver ~/.kube

$ kubectl create namespace falco
namespace/falco created



testserver ~/.kube
$ helm install falco -n falco --set falco.grpc.enabled=true --set falco.grpcOutput.enabled=true falcosecurity/falco
NAME: falco
LAST DEPLOYED: Mon Nov  9 14:20:55 2020
NAMESPACE: falco
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Falco agents are spinning up on each node in your cluster. After a few
seconds, they are going to start monitoring your containers looking for
security issues.
No further action should be required.

testserver ~/.kube
$ kubectl get namespaces
NAME                 STATUS   AGE
appconsolurlistio    Active   40d
argocd               Active   23d
default              Active   95d
ingress-controller   Active   95d
istio-system         Active   41d
kube-node-lease      Active   95d
kube-public          Active   95d
kube-system          Active   95d
rbac-manager         Active   95d
velero               Active   95d
falco                Active   103m    

testserver ~/.kube

$ kubectl get pod -n falco
NAME          READY   STATUS    RESTARTS   AGE
falco-nn47c   1/1     Running   0          2m20s
falco-q2clm   1/1     Running   0          2m20s
falco-rqf6r   1/1     Running   0          2m20s



$ helm install falco-exporter -n falco falcosecurity/falco-exporter
NAME: falco-exporter
LAST DEPLOYED: Mon Nov  9 17:42:58 2020
NAMESPACE: falco
STATUS: deployed
REVISION: 1
NOTES:
Get the falco-exporter metrics URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace falco -l "app.kubernetes.io/name=falco-exporter,app.kubernetes.io/instance=falco-exporter" -o jsonpath="{.items[0].metadata.name}")
  echo "Visit http://127.0.0.1:9376/metrics to use your application"
  kubectl port-forward --namespace falco $POD_NAME 9376




##############################################Uninstall falco#################################################################################################

$ helm uninstall falco -n falco
release "falco" uninstalled
kubectl create namespace falco

WeEu-S03-Tst-Aks-Aksc-99


testserver ~
$ kubectl get pod -n falco
NAME          READY   STATUS    RESTARTS   AGE
falco-4x7rz   1/1     Running   0          19m
falco-cgkfs   1/1     Running   0          19m
falco-vkqzb   1/1     Running   0          19m



$ kubectl exec -it falco-4x7rz ls -n falco
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
Unable to use a TTY - input is not a terminal or the right kind of file
bin
boot
dev
docker-entrypoint.sh
etc
home
host
lib
lib32
lib64
media
mnt
opt
proc
root
run
sbin
srv
sys
tmp
usr
var

$ kubectl exec -it falco-4x7rz cat etc/shadow  -n falco
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
Unable to use a TTY - input is not a terminal or the right kind of file
root:*:18513:0:99999:7:::
daemon:*:18513:0:99999:7:::
bin:*:18513:0:99999:7:::
sys:*:18513:0:99999:7:::
sync:*:18513:0:99999:7:::
games:*:18513:0:99999:7:::
man:*:18513:0:99999:7:::
lp:*:18513:0:99999:7:::
mail:*:18513:0:99999:7:::
news:*:18513:0:99999:7:::
uucp:*:18513:0:99999:7:::
proxy:*:18513:0:99999:7:::
www-data:*:18513:0:99999:7:::
backup:*:18513:0:99999:7:::
list:*:18513:0:99999:7:::
irc:*:18513:0:99999:7:::
gnats:*:18513:0:99999:7:::
nobody:*:18513:0:99999:7:::
_apt:*:18513:0:99999:7:::





$ kubectl get pod -n falco
NAME          READY   STATUS    RESTARTS   AGE
falco-4x7rz   1/1     Running   0          10s
falco-cgkfs   1/1     Running   0          10s
falco-vkqzb   1/1     Running   0          10s

testserver ~
$ kubectl logs -f falco-4x7rz  -n falco
* Setting up /usr/src links from host
* Running falco-driver-loader with: driver=module, compile=yes, download=yes
* Unloading falco module, if present
* Trying to dkms install falco module with GCC /usr/bin/gcc
DIRECTIVE: MAKE="'/tmp/falco-dkms-make'"

Kernel preparation unnecessary for this kernel.  Skipping...

Building module:
cleaning build area....
'/tmp/falco-dkms-make'........
cleaning build area....

DKMS: build completed.

falco.ko:
Running module version sanity check.

depmod...

DKMS: install completed.
* falco module installed in dkms, trying to insmod
* Success: falco module found and loaded in dkms
Tue Nov 10 10:36:20 2020: Falco version 0.26.1 (driver version 2aa88dcf6243982697811df4c1b484bcbe9488a2)
Tue Nov 10 10:36:20 2020: Falco initialized with configuration file /etc/falco/falco.yaml
Tue Nov 10 10:36:20 2020: Loading rules from file /etc/falco/falco_rules.yaml:
Tue Nov 10 10:36:20 2020: Loading rules from file /etc/falco/falco_rules.local.yaml:
Tue Nov 10 10:36:22 2020: Starting internal webserver, listening on port 8765

11:09:01.058023281: Warning Sensitive file opened for reading by non-trusted program (user=root user_loginuid=-1 program=cat command=cat etc/shadow file=/etc/shadow parent=<NA> gparent=<NA> ggparent=<NA> gggparent=<NA> container_id=b8e84d5d9d83 image=falcosecurity/falco) k8s.ns=falco k8s.pod=falco-4x7rz container=b8e84d5d9d83 k8s.ns=falco k8s.pod=falco-4x7rz container=b8e84d5d9d83




$ kubectl port-forward falco-4x7rz -n falco 8765:8765


#######################33 Contariner entrering ################33333



$ winpty kubectl exec -it falco-jbsx9 bash -n falco
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
root@falco-jbsx9:/# hostname
falco-jbsx9
root@falco-jbsx9:/#




################## Logstash ###########################################################################


$ kubectl create namespace logstash
namespace/logstash created



$ kubectl get namespace
NAME                 STATUS   AGE
appconsolurlistio    Active   41d
argocd               Active   25d
default              Active   96d
falco                Active   40h
ingress-controller   Active   96d
istio-system         Active   42d
kube-node-lease      Active   96d
kube-public          Active   96d
kube-system          Active   96d
logstash             Active   8s
pritam-test          Active   13h
rbac-manager         Active   96d
velero               Active   96d

 
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%% This log stach not required $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

$ helm repo add elastic https://helm.elastic.co
"elastic" has been added to your repositories

 



########### Logtash helm chart values.yml  update ############################## 

logstashConfig: 
  logstash.yml: |
    http.host: "0.0.0.0"
    path.config: /usr/share/logstash/pipeline  
#   key:
#   nestedkey: value
#  log4j2.properties: |
#    key = value
#
# Allows you to add any pipeline files in /usr/share/logstash/pipeline/
### ***warn*** there is a hardcoded logstash.conf in the image, override it first

logstashPipeline: 
  logstash.conf: |
    input {
       file {
          path => "/var/log/pods/falco*/*.log"
            }
         }
#     filter {
#         }    
        
#     output {
#        microsoft-logstash-output-azure-loganalytics {
#           workspace_id => d4e88b17-b48d-4cf9-91bd-0ee977596500
#           workspace_key => /Mvi3NX4s6SXnDT4BnqBWE698jJUbE2YkUcf40IrEVvClkPMY/IhxSwf0aD2NKV57QUYNFqg37OQzNbNWKlWBg==
#           custom_log_table_name => "falcoName"    
#          } 
#        }  
# Extra environment variables to append to this nodeGroup
# This will be appended to the current 'env:' key. You can use any of the kubernetes env
# syntax here
extraEnvs: []
#  - name: MY_ENVIRONMENT_VAR
#    value: the_value_goes_here

# Allows you to load environment variables from kubernetes secret or config map
#envFrom: [] 
envFrom: 
# - secretRef:
#     name: env-secret
  - configMapRef:
      name: logstash-configmap


####################################################################################


 helm install logstash . -n logging
$  helm upgrade logstash . -n logging
Release "logstash" has been upgraded. Happy Helming!
NAME: logstash
LAST DEPLOYED: Thu Nov 12 19:19:59 2020
NAMESPACE: logging
STATUS: deployed
REVISION: 6
TEST SUITE: None
NOTES:
1. Watch all cluster members come up.
  $ kubectl get pods --namespace=logging -l app=logstash-logstash -w



$ kubectl logs -f logstash-logstash-0 -n logging
Using bundled JDK: /usr/share/logstash/jdk
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
Sending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties
[2020-11-12T18:20:38,244][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"8.0.0", "jruby.version"=>"jruby 9.2.13.0 (2.5.7) 2020-08-03 9a89c94bcc OpenJDK 64-Bit Server VM 11.0.8+10 on 11.0.8+10 +indy +jit [linux-x86_64]"}
[2020-11-12T18:20:38,328][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>"path.queue", :path=>"/usr/share/logstash/data/queue"}
[2020-11-12T18:20:38,335][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>"path.dead_letter_queue", :path=>"/usr/share/logstash/data/dead_letter_queue"}
[2020-11-12T18:20:39,016][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-11-12T18:20:39,033][INFO ][logstash.agent           ] No persistent UUID file found. Generating new UUID {:uuid=>"b247d124-0db3-4b23-a51b-83c20f81ec74", :path=>"/usr/share/logstash/data/uuid"}
[2020-11-12T18:20:40,823][INFO ][org.reflections.Reflections] Reflections took 57 ms to scan 1 urls, producing 23 keys and 47 values
[2020-11-12T18:20:41,520][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>1, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>125, "pipeline.sources"=>["/usr/share/logstash/pipeline/logstash.conf"], :thread=>"#<Thread:0x69ba996e run>"}
[2020-11-12T18:20:41,531][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>0.01}
[2020-11-12T18:20:41,847][INFO ][logstash.inputs.file     ][main] No sincedb_path set, generating one based on the "path" setting {:sincedb_path=>"/usr/share/logstash/data/plugins/inputs/file/.sincedb_c6846d283e195bbeaf8d4aeeee83f97e", :path=>["/var/log/pods/falco*/sirene_update/*.log"]}
[2020-11-12T18:20:41,918][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2020-11-12T18:20:42,016][INFO ][filewatch.observingtail  ][main][fd940b8756e048de9dec0d7edd0767400914b644a3629ceeb17316e2ace69562] START, creating Discoverer, Watch with file and sincedb collections
[2020-11-12T18:20:42,031][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-11-12T18:20:42,419][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}





############################################### Log stash correct repo #################################################################


https://github.com/helm/charts/tree/master/stable/logstash

$ helm repo remove elastic 
"elastic" has been removed from your repositories






###################################################################################################################################





helm install falco -n falco \
  --set falco.grpc.enabled=true \
  --set falco.grpcOutput.enabled=true \
  --set falco.syslogOutput.enabled=true \
  --set falco.fileOutput.keepAlive=true \
  --set falco.fileOutput.enabled=true \
   --set falco.fileOutput.filename="/var/run/falco/falcolog" \
  --set falco.stdoutOutput.enabled=true \
  --set falco.jsonOutput=true \
  falcosecurity/falco
  
        helm upgrade --install falco --namespace falco \
      ./${{ parameters['folder'] }}/helmcharts/falco \
      -f ./${{ parameters['folder'] }}/helmcharts/falco/values.yaml
  
  
helm install falco -n falco \
  --set falco.grpc.enabled=true \
  --set falco.grpcOutput.enabled=true \
  --set falco.syslogOutput.enabled=true \
  --set falco.fileOutput.keepAlive=true \
  --set falco.fileOutput.enabled=true \
  --set falco.stdoutOutput.enabled=true \
  --set falco.jsonOutput=true \
  falcosecurity/falco
  
  
  
helm install falco . -n falco
  winpty kubectl exec -it falco-5l2xr bash -n falco


helm upgrade logstash . -n logging








################################################### Falco To LOGSTASH ################################################################
Logstash helm chart Value.yml configure:- 
Logstash Service expose to internal LB grab output from falco send to stdout:-
@@@@@@@@@@@@@@ Internal LB IP  10.232.69.251 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@22
service:
  type: LoadBalancer
  loadBalancerIP: 10.232.69.251
  annotations: 
    service.beta.kubernetes.io/azure-load-balancer-internal: "true" 
  ports:
    http: 
     port: 8080
     targetPort: http
     protocol: TCP
ports:
   - name: http
     containerPort: 8080
     protocol: TCP

inputs:
  main: |-
    input {
      http {
       host => "0.0.0.0" # default: 0.0.0.0
       port => 8080 # default: 8080
        }
    }

filters:
  # main: |-
  #   filter {
  #   }

outputs:
  main: |-
    output {
    
      stdout { codec => rubydebug }
      # microsoft-logstash-output-azure-loganalytics {
      #   workspace_id => d4e88b17-b48d-4cf9-91bd-0ee977596500
      #   workspace_key => /Mvi3NX4s6SXnDT4BnqBWE698jJUbE2YkUcf40IrEVvClkPMY/IhxSwf0aD2NKV57QUYNFqg37OQzNbNWKlWBg==
      #   custom_log_table_name => "falcoName"    
      # }

    }

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
$ kubectl get svc -n logging
NAME       TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE
logstash   LoadBalancer   172.20.178.67   10.232.69.251   8080:31712/TCP   142m


####################################################################################################################################

Falco Helm chart value.yaml to send output to logstash
 
 httpOutput:
    enabled: true
    url: http://10.232.69.251:8080



#####################################################################################################################################
https://github.com/helm/charts

TEST by sending value by REST API
curl -XPUT 'http://10.232.69.251:8080/twitter/tweet/1' -d 'hello'



$ kubectl get pod -n logging
NAME         READY   STATUS    RESTARTS   AGE
logstash-0   1/1     Running   0          47m


$ kubectl logs -f logstash-0 -n logging
2020/11/15 09:27:55 Setting 'queue.max_bytes' from environment.
2020/11/15 09:27:55 Setting 'path.config' from environment.
2020/11/15 09:27:55 Setting 'queue.drain' from environment.
2020/11/15 09:27:55 Setting 'http.port' from environment.
2020/11/15 09:27:55 Setting 'http.host' from environment.
2020/11/15 09:27:55 Setting 'path.data' from environment.
2020/11/15 09:27:55 Setting 'queue.checkpoint.writes' from environment.
2020/11/15 09:27:55 Setting 'queue.type' from environment.
2020/11/15 09:27:55 Setting 'config.reload.automatic' from environment.
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
Sending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties
[2020-11-15T09:28:07,071][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-11-15T09:28:07,087][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.1.1"}
[2020-11-15T09:28:12,126][INFO ][logstash.javapipeline    ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>1, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>125, :thread=>"#<Thread:0x4872d64b run>"}
[2020-11-15T09:28:12,440][INFO ][logstash.javapipeline    ] Pipeline started {"pipeline.id"=>"main"}
[2020-11-15T09:28:12,469][INFO ][logstash.inputs.http     ] Starting http input listener {:address=>"0.0.0.0:8080", :ssl=>"false"}
[2020-11-15T09:28:12,599][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-11-15T09:28:12,871][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/awesome_print-1.7.0/lib/awesome_print/formatters/base_formatter.rb:31: warning: constant ::Fixnum is deprecated
{
      "@version" => "1",
    "@timestamp" => 2020-11-15T09:29:50.545Z,
          "host" => "10.232.69.228",
       "headers" => {
         "content_length" => "5",
              "http_host" => "10.232.69.251:8080",
           "request_path" => "/twitter/tweet/1",
           "http_version" => "HTTP/1.1",
        "http_user_agent" => "curl/7.73.0",
            "http_accept" => "*/*",
           "content_type" => "application/x-www-form-urlencoded",
         "request_method" => "PUT"
    },
       "message" => "hello"
}
{
           "output" => "10:01:57.131417000: Notice Privileged container started (user=<NA> user_loginuid=0 command=container:2f9ee7d74eae k8s.ns=kube-system k8s.pod=calico-node-9sdzq container=2f9ee7d74eae image=mcr.microsoft.com/oss/calico/node:v3.8.0) k8s.ns=kube-system k8s.pod=calico-node-9sdzq container=2f9ee7d74eae",
         "priority" => "Notice",
         "@version" => "1",
             "time" => "2020-11-15T10:01:57.131417000Z",
       "@timestamp" => 2020-11-15T10:02:01.252Z,
             "host" => "10.232.69.231",
    "output_fields" => {
               "container.image.tag" => "v3.8.0",
                      "proc.cmdline" => "container:2f9ee7d74eae",
                     "user.loginuid" => 0,
                      "container.id" => "2f9ee7d74eae",
                          "evt.time" => 1605434517131417000,
                         "user.name" => nil,
        "container.image.repository" => "mcr.microsoft.com/oss/calico/node",
                       "k8s.ns.name" => "kube-system",
                      "k8s.pod.name" => "calico-node-9sdzq"
    },
          "headers" => {
         "content_length" => "722",
              "http_host" => "10.232.69.251:8080",
           "request_path" => "/",
           "http_version" => "HTTP/1.1",
        "http_user_agent" => nil,
            "http_accept" => "*/*",
           "content_type" => "application/json",
         "request_method" => "POST"
    },
             "rule" => "Launch Privileged Container"
}



$kubectl exec logstash-0  bin/logstash-plugin list  -n logging  > logtashplugin.txt
logstash-codec-avro
logstash-codec-cef
logstash-codec-collectd
logstash-codec-dots
logstash-codec-edn
logstash-codec-edn_lines
logstash-codec-es_bulk
logstash-codec-fluent
logstash-codec-graphite
logstash-codec-json
logstash-codec-json_lines
logstash-codec-line
logstash-codec-msgpack
logstash-codec-multiline
logstash-codec-netflow
logstash-codec-plain
logstash-codec-rubydebug
logstash-filter-aggregate
logstash-filter-anonymize
logstash-filter-cidr
logstash-filter-clone
logstash-filter-csv
logstash-filter-date
logstash-filter-de_dot
logstash-filter-dissect
logstash-filter-dns
logstash-filter-drop
logstash-filter-elasticsearch
logstash-filter-fingerprint
logstash-filter-geoip
logstash-filter-grok
logstash-filter-http
logstash-filter-jdbc_static
logstash-filter-jdbc_streaming
logstash-filter-json
logstash-filter-kv
logstash-filter-memcached
logstash-filter-metrics
logstash-filter-mutate
logstash-filter-prune
logstash-filter-ruby
logstash-filter-sleep
logstash-filter-split
logstash-filter-syslog_pri
logstash-filter-throttle
logstash-filter-translate
logstash-filter-truncate
logstash-filter-urldecode
logstash-filter-useragent
logstash-filter-uuid
logstash-filter-xml
logstash-input-azure_event_hubs
logstash-input-beats
logstash-input-couchdb_changes
logstash-input-dead_letter_queue
logstash-input-elasticsearch
logstash-input-exec
logstash-input-file
logstash-input-ganglia
logstash-input-gelf
logstash-input-generator
logstash-input-graphite
logstash-input-heartbeat
logstash-input-http
logstash-input-http_poller
logstash-input-imap
logstash-input-jdbc
logstash-input-kafka
logstash-input-pipe
logstash-input-rabbitmq
logstash-input-redis
logstash-input-s3
logstash-input-snmp
logstash-input-snmptrap
logstash-input-sqs
logstash-input-stdin
logstash-input-syslog
logstash-input-tcp
logstash-input-twitter
logstash-input-udp
logstash-input-unix
logstash-output-cloudwatch
logstash-output-csv
logstash-output-elastic_app_search
logstash-output-elasticsearch
logstash-output-email
logstash-output-file
logstash-output-graphite
logstash-output-http
logstash-output-kafka
logstash-output-lumberjack
logstash-output-nagios
logstash-output-null
logstash-output-pipe
logstash-output-rabbitmq
logstash-output-redis
logstash-output-s3
logstash-output-sns
logstash-output-sqs
logstash-output-stdout
logstash-output-tcp
logstash-output-udp
logstash-output-webhdfs
logstash-patterns-core




$ kubectl exec logstash-0  bin/logstash-plugin install logstash-output-azure_loganalytics -n logging
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC treads appropriately using -XX:ParallelGCThreads=N
Validating logstash-output-azure_loganalytics
Installing logstash-output-azure_loganalytics
Installation successful
$ kubectl exec logstash-0  bin/logstash-plugin list  -n logging | grep azure
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
logstash-input-azure_event_hubs
logstash-output-azure_loganalytics

kubectl exec logstash-0  bin/logstash-plugin install microsoft-logstash-output-azure-loganalytics






$ kubectl exec logstash-0 -n logging -- printenv | grep SERVICE
KUBERNETES_SERVICE_HOST=172.20.0.1
LOGSTASH_SERVICE_PORT=8080
LOGSTASH_SERVICE_PORT_HTTP=8080
LOGSTASH_SERVICE_HOST=172.20.39.245
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443

$ kubectl get services kube-dns --namespace=kube-system
NAME       TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   172.20.0.10   <none>        53/UDP,53/TCP   102d


$ kubectl describe svc -n logging
Name:              logstash
Namespace:         logging
Labels:            app=logstash
                   app.kubernetes.io/managed-by=Helm
                   chart=logstash-2.4.2
                   heritage=Helm
                   release=logstash
Annotations:       external-dns.alpha.kubernetes.io/hostname: logstash.cluster.local
                   meta.helm.sh/release-name: logstash
                   meta.helm.sh/release-namespace: logging
Selector:          app=logstash,release=logstash
Type:              ClusterIP
IP:                172.20.39.245
Port:              http  8080/TCP
TargetPort:        http/TCP
Endpoints:         172.19.3.68:8080
Session Affinity:  None
Events:            <none>


$ winpty kubectl run curl1 --image=radial/busyboxplus:curl -i --tty
If you don't see a command prompt, try pressing enter.
[ root@curl1:/ ]$ nslookup logtash
Server:    172.20.0.10
Address 1: 172.20.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'logtash'
[ root@curl1:/ ]$ nslookup logstash
Server:    172.20.0.10
Address 1: 172.20.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'logstash'
[ root@curl1:/ ]$




 grace=$(kubectl get pods logstash-0 -n logging --template '{{.spec.terminationGracePeriodSeconds}}')


$ kubectl get statefulset -n logging
NAME       READY   AGE
logstash   1/1     73s


 kubectl delete  statefulset -n logging -l app=logstash
 
 
 $ kubectl get pvc -l app=logstash -n logging
NAME                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-logstash-0        Bound    pvc-47ad9349-e0aa-4141-90f6-b02672d49b8f   2Gi        RWO            default        3h
data-logstashfalco-0   Bound    pvc-46729496-3ea1-4488-9440-bc0f547db1c2   2Gi        RWO            default        3h

testserv ~/Desktop/logtash/logtash
$ kubectl delete pvc -l app=logstash -n logging
persistentvolumeclaim "data-logstash-0" deleted
persistentvolumeclaim "data-logstashfalco-0" deleted




###################### LOGSTSH PLUGIN INSTALL by HELM EDIT ######################################################################### 
statefulset.yaml


        ## logstash
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          {{- if .Values.plugin }}
          command: ["/bin/sh", "-c"]
          args: ["/usr/share/logstash/bin/logstash-plugin install {{ .Values.plugin }} ; /usr/local/bin/docker-entrypoint"]
          {{- end }}
values.yaml

image:
  repository: docker.elastic.co/logstash/logstash-oss
  tag: 7.1.1
  pullPolicy: Always
plugin: "microsoft-logstash-output-azure-loganalytics"

 inputs:
  main: |-
    input {
       http {
       host => "0.0.0.0" # default: 0.0.0.0
       port => 8080 # default: 8080
        }
    }
filters:
  # main: |-
  #   filter {
  #   }

outputs:
  main: |-
    output {
    
      stdout { codec => rubydebug }
       microsoft-logstash-output-azure-loganalytics {
          workspace_id => "TTTTTTTTTTTTTTTTTTTTTTTttt"
          workspace_key => "/MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM"
          custom_log_table_name => "falcoName"    
       }
       #azure_loganalytics  {
       #  workspace_id => "TTTTTTTTTTTTTTTTTTTTTTTttt"
       #  workspace_key => "/MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM"
       #  custom_log_table_name => "falcoName"    
       #}
    }

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 




$ kubectl logs -f  logstash-0  -n logging
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
Installing microsoft-logstash-output-azure-loganalytics
Installation successful
2020/11/18 10:41:09 Setting 'path.config' from environment.
2020/11/18 10:41:09 Setting 'queue.max_bytes' from environment.
2020/11/18 10:41:09 Setting 'queue.drain' from environment.
2020/11/18 10:41:09 Setting 'http.port' from environment.
2020/11/18 10:41:09 Setting 'http.host' from environment.
2020/11/18 10:41:09 Setting 'path.data' from environment.
2020/11/18 10:41:09 Setting 'queue.checkpoint.writes' from environment.
2020/11/18 10:41:09 Setting 'queue.type' from environment.
2020/11/18 10:41:09 Setting 'config.reload.automatic' from environment.
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
Sending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties
[2020-11-18T10:41:21,479][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-11-18T10:41:21,492][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.1.1"}
[2020-11-18T10:41:27,443][INFO ][logstash.outputs.azureloganalytics] Azure Loganalytics configuration was found valid.
[2020-11-18T10:41:27,444][INFO ][logstash.outputs.azureloganalytics] Logstash Azure Loganalytics output plugin configuration was found valid
[2020-11-18T10:41:27,553][INFO ][logstash.javapipeline    ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>1, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>125, :thread=>"#<Thread:0x69699d24 run>"}
[2020-11-18T10:41:27,805][INFO ][logstash.javapipeline    ] Pipeline started {"pipeline.id"=>"main"}
[2020-11-18T10:41:27,825][INFO ][logstash.inputs.http     ] Starting http input listener {:address=>"0.0.0.0:8080", :ssl=>"false"}
[2020-11-18T10:41:27,885][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-11-18T10:41:28,217][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/awesome_print-1.7.0/lib/awesome_print/formatters/base_formatter.rb:31: warning: constant ::Fixnum is deprecated
{
           "output" => "Falco internal: syscall event drop. 2 system calls dropped in last second.",
             "rule" => "Falco internal: syscall event drop",
          "headers" => {
           "http_version" => "HTTP/1.1",
         "content_length" => "317",
            "http_accept" => "*/*",
           "request_path" => "/",
           "content_type" => "application/json",
         "request_method" => "POST",
              "http_host" => "logstash.logging:8080",
        "http_user_agent" => nil
    },
             "host" => "172.19.4.73",
         "@version" => "1",
         "priority" => "Critical",
       "@timestamp" => 2020-11-18T10:41:37.369Z,
    "output_fields" => {
        "n_drops_buffer" => "2",
          "ebpf_enabled" => "0",
               "n_drops" => "2",
           "n_drops_bug" => "0",
            "n_drops_pf" => "0",
                "n_evts" => "25816"
    },
             "time" => "2020-11-18T10:41:37.217570175Z"
}
[2020-11-18T10:41:37,858][INFO ][logstash.outputs.azureloganalytics] Changing buffer size.[configuration='2000' , new_size='1900']
[2020-11-18T10:41:39,148][INFO ][logstash.outputs.azureloganalytics] Successfully posted 1 logs into custom log analytics table[falcoName].



$ az monitor log-analytics workspace table show --resource-group weeu-subs-tnd-rsg-mn-01 --workspace-name Weeu-subs-tnd-La-01 -n "falcoName_CL"
{
  "id": "/subscriptions/XXXXXXXXXXXXXXXXXXXXXXX/resourcegroups/weeu-subs-tnd-rsg-mn-01/providers/Microsoft.OperationalInsights/workspaces/weeu-subs-tnd-la-01/tables/falcoName_CL",
  "name": "falcoName_CL",
  "resourceGroup": "weeu-subs-tnd-rsg-mn-01",
  "retentionInDays": null,
  "type": null
}

